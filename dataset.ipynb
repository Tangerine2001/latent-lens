{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['secret'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "\n",
    "with open('keys.json', \"r\") as f:\n",
    "    keys = json.load(f)\n",
    "print(keys.keys())\n",
    "\n",
    "openai.api_key = keys['secret']\n",
    "\n",
    "dataDir = \"datasets/\"\n",
    "with open(dataDir + \"counterfact.json\", \"r\") as f:\n",
    "    counterfact = json.load(f)\n",
    "if os.path.exists(dataDir + \"related.json\"):\n",
    "    with open(dataDir +\"related.json\", \"r\") as f:\n",
    "        related = json.load(f)\n",
    "else:\n",
    "    related = []\n",
    "print(len(counterfact))\n",
    "print(len(related))\n",
    "totalTokens = 0\n",
    "\n",
    "def costEstimate(tokens):\n",
    "    return (tokens/1000) * 0.002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genMsg(idx):\n",
    "    msg = counterfact[idx]['requested_rewrite']['prompt']\n",
    "    msg = msg.replace(\"{}\", counterfact[idx]['requested_rewrite']['subject'])\n",
    "    msg = msg + \" \" + counterfact[idx]['requested_rewrite']['target_true'][\"str\"] + \".\"\n",
    "    msg = msg + f\" Can you write a list on one line without commas of related words to the subject of the first sentence, {counterfact[idx]['requested_rewrite']['subject']}.\"\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genResponse(msg):\n",
    "    global totalTokens\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": msg}\n",
    "        ],\n",
    "        max_tokens=20,\n",
    "        # stop=[\"\\n\", \".\"],\n",
    "        temperature=0.8,\n",
    "        presence_penalty=0.0,\n",
    "    )\n",
    "    totalTokens += response[\"usage\"][\"total_tokens\"]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = [\".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\"]\n",
    "def postProcess(words):\n",
    "    for punc in punctuation:\n",
    "        words = words.replace(punc, \" \")\n",
    "    words = words.split(\" \")\n",
    "    words = [word for word in words if word != \"\"]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Michael Einhorn\\Documents\\MLProjects\\latent-lens\\dataset.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/MLProjects/latent-lens/dataset.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(related)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/MLProjects/latent-lens/dataset.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(counterfact[i])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/MLProjects/latent-lens/dataset.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m msg \u001b[39m=\u001b[39m genMsg(i)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/MLProjects/latent-lens/dataset.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(msg)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "i = len(related)\n",
    "print(counterfact[i])\n",
    "msg = genMsg(i)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = genResponse(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "# print(postProcess(response[\"choices\"][0][\"message\"][\"content\"]))\n",
    "# print(response[\"choices\"][0][\"finish_reason\"])\n",
    "# print(response[\"usage\"][\"total_tokens\"])\n",
    "# print(costEstimate(totalTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21918 / 21919 Current Cost 1.724052000000000235\r"
     ]
    }
   ],
   "source": [
    "testing = False\n",
    "\n",
    "for i in range(len(related), len(counterfact)):\n",
    "    msg = genMsg(i)\n",
    "    response = genResponse(msg)\n",
    "    words = postProcess(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "    if testing:\n",
    "        print(msg)\n",
    "        print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "        print(words)\n",
    "\n",
    "    entry = counterfact[i]\n",
    "    entry[\"relatedWords\"] = words\n",
    "    entry[\"responseRaw\"] = response.to_dict()\n",
    "    related.append(entry)\n",
    "    with open(dataDir + \"related.json\", \"w\") as f:\n",
    "        json.dump(related, f)\n",
    "\n",
    "    # print(response[\"choices\"][0][\"finish_reason\"])\n",
    "    print(f\"{i} / {len(counterfact)} Current Cost {costEstimate(totalTokens)}\", end=\"\\r\")\n",
    "    if costEstimate(totalTokens) > 10:\n",
    "        break\n",
    "\n",
    "    if testing:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['case_id', 'pararel_idx', 'requested_rewrite', 'paraphrase_prompts', 'neighborhood_prompts', 'attribute_prompts', 'generation_prompts', 'relatedWords', 'responseRaw'])\n",
      "['actor', 'theatre', 'film', 'stage', 'performance', 'drama', 'comedy', 'tragedy', 'audition', 'rehearsal', 'script', 'character', 'role', 'director', 'producer', 'cinematographer', 'screenplay', 'set', 'design']\n",
      "21919\n"
     ]
    }
   ],
   "source": [
    "print(related[-1].keys())\n",
    "print(related[-1][\"relatedWords\"])\n",
    "print(len(related))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import IterableDataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch as th\n",
    "from torchinfo import summary\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from core import padded_stack, stack_dicts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21919\n"
     ]
    }
   ],
   "source": [
    "dataDir = \"datasets/\"\n",
    "with open(dataDir +\"related.json\", \"r\") as f:\n",
    "    relatedJson = json.load(f)\n",
    "\n",
    "print(len(relatedJson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json with strings to tuple of tensors\n",
    "class RelatedDataset(Dataset):\n",
    "    def __init__(self, dataJson, tokenizer, pad_to_multiple_of=1):\n",
    "        self.dataJson = dataJson\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        self.maxLenPrompt = 0\n",
    "        self.maxLenResponse = 0\n",
    "        self.maxLenRelated = 0\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataJson)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        prompt, response, related = self.formatText(index)\n",
    "\n",
    "        promptT = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
    "        responseT = self.tokenizer(response, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
    "        relatedT = self.tokenizer(related, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        return promptT, responseT, relatedT\n",
    "\n",
    "    def formatText(self, idx):\n",
    "        # print(\"data index\", idx)\n",
    "        prompt = self.dataJson[idx]['requested_rewrite']['prompt']\n",
    "        prompt = prompt.replace(\"{}\", self.dataJson[idx]['requested_rewrite']['subject'])\n",
    "        response = self.dataJson[idx]['requested_rewrite']['target_true'][\"str\"] + \".\"\n",
    "        related = self.dataJson[idx]['relatedWords']\n",
    "        relatedStr = related[0]\n",
    "        for r in related[1:]:\n",
    "            relatedStr += \" \" + r\n",
    "\n",
    "        relatedStr = relatedStr.replace(\".\",\"\")\n",
    "        relatedStr = relatedStr.replace(\",\",\"\")\n",
    "        relatedStr = relatedStr.replace(\"  \",\" \")\n",
    "\n",
    "        return prompt, response, relatedStr\n",
    "    \n",
    "# batching and padding, use left pad tokkenizer\n",
    "class RelatedCollator():\n",
    "    def __init__(self, tokenizer, pad_to_multiple_of=1):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.textCollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=pad_to_multiple_of)\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        prompt, response, related = zip(*batch)\n",
    "\n",
    "        # print(prompt)\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        prompt = self.textCollator(prompt)\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        response = self.textCollator(response)\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        related = self.textCollator(related)\n",
    "\n",
    "        # causing index out of range error\n",
    "        del prompt[\"labels\"]\n",
    "        del response[\"labels\"]\n",
    "        del related[\"labels\"]\n",
    "\n",
    "        prompt[\"attention_mask\"] = th.where(prompt[\"input_ids\"] == self.pad_token_id, 0, 1)\n",
    "        response[\"attention_mask\"] = th.where(response[\"input_ids\"] == self.pad_token_id, 0, 1)\n",
    "        related[\"attention_mask\"] = th.where(related[\"input_ids\"] == self.pad_token_id, 0, 1)\n",
    "\n",
    "        return prompt, response, related # token index in [\"input_ids\"], -100 for pad in [\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "# left pad so all generations start at the same index\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side='left', return_special_tokens_mask=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "print(tokenizer.pad_token_id)\n",
    "dataset = RelatedDataset(relatedJson, tokenizer)\n",
    "collator = RelatedCollator(tokenizer, dataset.pad_to_multiple_of)\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collator, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "2\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "maxLenPrompt = 0\n",
    "maxLenResponse = 0\n",
    "maxLenRelated = 0\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    prompt, response, related = dataset[i]\n",
    "    maxLenPrompt = max(maxLenPrompt, prompt.shape[0])\n",
    "    maxLenResponse = max(maxLenResponse, response.shape[0])\n",
    "    maxLenRelated = max(maxLenRelated, related.shape[0])\n",
    "    break\n",
    "\n",
    "print(maxLenPrompt)\n",
    "print(maxLenResponse)\n",
    "print(maxLenRelated)\n",
    "\n",
    "dataset.maxLenPrompt = maxLenPrompt\n",
    "dataset.maxLenResponse = maxLenResponse\n",
    "dataset.maxLenRelated = maxLenRelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = RelatedCollator(tokenizer, dataset.pad_to_multiple_of)\n",
    "dl = DataLoader(dataset, batch_size=2, collate_fn=collator, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt {'input_ids': tensor([[32117, 44414,   280,    11, 10834,   286],\n",
      "        [   33,   363, 25791,   293,   318,   287]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1]])}\n",
      "reponse {'input_ids': tensor([[24111,    13, 50256],\n",
      "        [16748,  5439,    13]]), 'attention_mask': tensor([[1, 1, 0],\n",
      "        [1, 1, 1]])}\n",
      "related {'input_ids': tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 34470,  6749, 28278,\n",
      "          6853,  6260, 18026, 18026],\n",
      "        [16748,  5439,  7072,  3734, 17423, 22158, 33072, 27407,  2815,  3491,\n",
      "         15162, 24800, 40250,  1570]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "for prompt, response, related in dl:\n",
    "    print(\"prompt\", prompt)\n",
    "    print(\"reponse\", response)\n",
    "    print(\"related\", related)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tuned_lens.nn.lenses import TunedLens, LogitLens\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = th.device('cuda')\n",
    "loadModel = False\n",
    "modelName = 'gpt2'\n",
    "# modelName = 'sshleifer/tiny-gpt2'\n",
    "# To try a diffrent modle / lens check if the lens is avalible then modify this code\n",
    "model = AutoModelForCausalLM.from_pretrained(modelName)\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "if loadModel:\n",
    "    tuned_lens = TunedLens.load(modelName, map_location=device)\n",
    "else:\n",
    "    tuned_lens = TunedLens(model)\n",
    "tuned_lens = tuned_lens.to(device)\n",
    "logit_lens = LogitLens(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "windows = False\n",
    "try:\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "except:\n",
    "  if os.name == 'nt':\n",
    "    windows = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "GPT2LMHeadModel                                    --\n",
      "├─GPT2Model: 1-1                                   --\n",
      "│    └─Embedding: 2-1                              38,597,376\n",
      "│    └─Embedding: 2-2                              786,432\n",
      "│    └─Dropout: 2-3                                --\n",
      "│    └─ModuleList: 2-4                             --\n",
      "│    │    └─GPT2Block: 3-1                         7,087,872\n",
      "│    │    └─GPT2Block: 3-2                         7,087,872\n",
      "│    │    └─GPT2Block: 3-3                         7,087,872\n",
      "│    │    └─GPT2Block: 3-4                         7,087,872\n",
      "│    │    └─GPT2Block: 3-5                         7,087,872\n",
      "│    │    └─GPT2Block: 3-6                         7,087,872\n",
      "│    │    └─GPT2Block: 3-7                         7,087,872\n",
      "│    │    └─GPT2Block: 3-8                         7,087,872\n",
      "│    │    └─GPT2Block: 3-9                         7,087,872\n",
      "│    │    └─GPT2Block: 3-10                        7,087,872\n",
      "│    │    └─GPT2Block: 3-11                        7,087,872\n",
      "│    │    └─GPT2Block: 3-12                        7,087,872\n",
      "│    └─LayerNorm: 2-5                              1,536\n",
      "├─Linear: 1-2                                      38,597,376\n",
      "===========================================================================\n",
      "Total params: 124,439,808\n",
      "Trainable params: 124,439,808\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "TunedLens                                --\n",
      "├─Sequential: 1-1                        --\n",
      "├─Linear: 1-2                            (38,597,376)\n",
      "├─LayerNorm: 1-3                         (1,536)\n",
      "├─Linear: 1-4                            590,592\n",
      "├─ModuleList: 1-5                        --\n",
      "│    └─Linear: 2-1                       590,592\n",
      "│    └─Linear: 2-2                       590,592\n",
      "│    └─Linear: 2-3                       590,592\n",
      "│    └─Linear: 2-4                       590,592\n",
      "│    └─Linear: 2-5                       590,592\n",
      "│    └─Linear: 2-6                       590,592\n",
      "│    └─Linear: 2-7                       590,592\n",
      "│    └─Linear: 2-8                       590,592\n",
      "│    └─Linear: 2-9                       590,592\n",
      "│    └─Linear: 2-10                      590,592\n",
      "│    └─Linear: 2-11                      590,592\n",
      "=================================================================\n",
      "Total params: 45,686,016\n",
      "Trainable params: 7,087,104\n",
      "Non-trainable params: 38,598,912\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "print(summary(model))\n",
    "print(summary(tuned_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt {'input_ids': tensor([[38743,  5326,    11,   508,  5341,   262,  2292],\n",
      "        [50256, 50256, 50256, 34979, 46658, 20973,   287]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1]], device='cuda:0')}\n",
      "reponse {'input_ids': tensor([[24385,  1891,    13],\n",
      "        [31710,    13, 50256]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1],\n",
      "        [1, 1, 0]], device='cuda:0')}\n",
      "related {'input_ids': tensor([[15914, 16076,  3554,  3714,   263,  1074,  2137, 10242, 30286,  6737,\n",
      "         24955,  2214],\n",
      "        [50256, 31710,  2647,  4097,  2854, 32010,  3223, 16212,  8984, 28201,\n",
      "         23689, 46644]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "torch.Size([2, 7])\n",
      "torch.Size([2, 7])\n",
      "dict_keys(['logits', 'past_key_values'])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "dict_keys(['logits', 'past_key_values'])\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "from tuned_lens.utils import (\n",
    "    maybe_all_reduce,\n",
    "    shift_labels,\n",
    "    shift_preds,\n",
    "    send_to_device,\n",
    ")\n",
    "\n",
    "from tuned_lens.residual_stream import ResidualStream, record_residual_stream\n",
    "with th.no_grad():\n",
    "    for batch_t in dl:\n",
    "        batch_t = send_to_device(batch_t, device)\n",
    "        # batch is left pad, response is right pad, related is left pad\n",
    "        # _ _ _ _ prompt | response _ _ _\n",
    "        batch, response, related = batch_t\n",
    "        print(\"prompt\", batch)\n",
    "        print(\"reponse\", response)\n",
    "        print(\"related\", related)\n",
    "\n",
    "        print(batch[\"input_ids\"].shape)\n",
    "        print(batch[\"attention_mask\"].shape)\n",
    "        \n",
    "        \n",
    "        with record_residual_stream(model) as stream:\n",
    "            output = model(**batch)\n",
    "            print(dict(output).keys())\n",
    "        \n",
    "        print(response[\"input_ids\"].shape)\n",
    "        print(response[\"attention_mask\"].shape)\n",
    "        \n",
    "        # needs an attention mask for full prompt, but input ids dont include past keys, logits are just for response tokens\n",
    "        full_att = th.cat([batch[\"attention_mask\"], response[\"attention_mask\"]], dim=1)\n",
    "        responseOutput = model(input_ids=response[\"input_ids\"], attention_mask=full_att, past_key_values=output.past_key_values)\n",
    "\n",
    "        print(dict(responseOutput).keys())\n",
    "        print(full_att.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
